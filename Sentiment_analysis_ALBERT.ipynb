{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dPWOnLw_6Ogha58mA91BxeIFW3Hm9AvK",
      "authorship_tag": "ABX9TyMZOzrONXq2AncOaLEVZH7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dreadnought73/AI_projects/blob/main/Sentiment_analysis_ALBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis with ALBERT"
      ],
      "metadata": {
        "id": "wZuGST2n26TN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWL7utlERO1u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification, AlbertConfig\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finance = pd.read_csv('/content/drive/MyDrive/Coding_data_files/finance_sentiment_analysis.csv', encoding='latin-1')\n",
        "finance.head()"
      ],
      "metadata": {
        "id": "rsHSmd4FRcT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting new columns for easier accessibility during the development."
      ],
      "metadata": {
        "id": "icjswWsB3NnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finance.columns = ['Sentiment', 'Text']"
      ],
      "metadata": {
        "id": "eQrfgbRMSx73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finance.head()"
      ],
      "metadata": {
        "id": "ySzX7vldVjUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neutral category has an overwhelming majority compared to the other categories."
      ],
      "metadata": {
        "id": "71AEAKlD3g6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finance['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "4zSLDsafYCNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "finance['encoded_sentiments'] = label_encoder.fit_transform(finance['Sentiment'])\n",
        "\n",
        "# Map encoded labels back to sentiment names for reference\n",
        "# This creates a dictionary to easily look up the sentiment name from the encoded integer\n",
        "encoded_to_label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "print(f\"Encoded labels mapping: {encoded_to_label}\")"
      ],
      "metadata": {
        "id": "aabw1n93bT8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    finance['Text'],\n",
        "    finance['encoded_sentiments'],\n",
        "    test_size=.2,\n",
        "    random_state=42,\n",
        "    stratify=finance['encoded_sentiments']\n",
        ")\n",
        "\n",
        "print(f\"\\nLoading ALBERT tokenizer and model: {'albert-base-v2'}\")\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "\n",
        "config = AlbertConfig.from_pretrained('albert-base-v2', num_labels=3)\n",
        "model = TFAlbertForSequenceClassification.from_pretrained('albert-base-v2', config=config)"
      ],
      "metadata": {
        "id": "eY3rt4Qhcs55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is to find the optimal max_seq_length for the final tokenizer"
      ],
      "metadata": {
        "id": "X2fMDdSnrwiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(\n",
        "    list(finance['Text'].values),\n",
        "    truncation=False,\n",
        "    padding=False,\n",
        "    return_attention_mask=False,\n",
        "    return_token_type_ids=False,\n",
        "    return_tensors=None\n",
        ")\n",
        "\n",
        "token_lengths = [len(input_ids) for input_ids in encoded_inputs['input_ids']]\n",
        "\n",
        "print(f\"Calculated lengths for {len(token_lengths)} sentences.\")"
      ],
      "metadata": {
        "id": "QFwka6A3rWdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding out where most the current sentences end to know what max_length to set."
      ],
      "metadata": {
        "id": "TJHWBFf558zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAnalyzing token length distribution:\")\n",
        "print(f\"  Min Length: {np.min(token_lengths)}\")\n",
        "print(f\"  Max Length: {np.max(token_lengths)}\")\n",
        "print(f\"  Mean Length: {np.mean(token_lengths):.2f}\")\n",
        "print(f\"  Median Length: {np.median(token_lengths)}\")\n",
        "\n",
        "\n",
        "percentiles = [90, 95, 99]\n",
        "for p in percentiles:\n",
        "    length = np.percentile(token_lengths, p)\n",
        "    print(f\"  {p}th Percentile Length: {length:.2f}\")"
      ],
      "metadata": {
        "id": "3YpJJUZlroDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the train and test set and creating constant tensors."
      ],
      "metadata": {
        "id": "4O-g1Qmk6VF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoding = tokenizer(\n",
        "    list(X_train.values),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=63,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "test_encoding = tokenizer(\n",
        "    list(X_test.values),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=63,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "y_train_tf = tf.constant(y_train.values)\n",
        "y_test_tf = tf.constant(y_test.values)"
      ],
      "metadata": {
        "id": "Z1S4PbAEcs2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building and training the model."
      ],
      "metadata": {
        "id": "iW2Psxoi6ndS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # loss function\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy') # defining the metrics for evaluation\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics) # putting everything together"
      ],
      "metadata": {
        "id": "LGE9WQLucs0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it pairs up each sentence's tokenized encodings (as a dictionary) with its corresponding label (tf.data.Dataset.from_tensor_slices)\n",
        "print(\"\\nFinetuning the ALBERT model...\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encoding), # dictionary containing tensors like input_ids, attention_masks:\n",
        "                          # Each \"slice\" from this will be a dictionary containing one sequence of input IDs, one attention mask, etc., corresponding to a single sentence\n",
        "    y_train_tf)).batch(24)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encoding), y_test_tf)).batch(24)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=5,\n",
        "    validation_data=test_dataset\n",
        "    )\n",
        "\n",
        "print(\"Model finetuning complete.\")"
      ],
      "metadata": {
        "id": "UnIlUbwTcsyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hp57lFUHcswD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "h--FINuOcsoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentences = [\n",
        "    \"The stock market saw mixed movements this week as investors digested recent economic data. Trading volume remained moderate across major exchanges.\",\n",
        "    \"Strong job growth numbers indicate a strengthening economy. This is a positive sign for future economic expansion.\",\n",
        "    \"Supply chain disruptions continue to weigh on manufacturing output. This could impact inventory levels and consumer prices.\",\n",
        "    \"Inflation figures were released today, showing a slight change from the previous month. Analysts are considering the potential impact on consumer spending.\",\n",
        "    \"Several companies reported better-than-expected earnings this quarter. This performance is boosting confidence in those sectors.\",\n",
        "    \"Geopolitical tensions are creating uncertainty in global markets. Investors are showing caution due to increased risks.\",\n",
        "    \"A key economic indicator showed a contraction in the last quarter. This has raised concerns about the pace of economic recovery.\",\n",
        "    \"Market sentiment improved today with a broad rally across technology stocks. Innovation continues to drive growth in the sector.\",\n",
        "    \"The central bank announced its decision on interest rates this afternoon. Rates will remain unchanged for the current period.\"\n",
        "]\n",
        "\n",
        "sample_encodings = tokenizer(\n",
        "    sample_sentences,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=70,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "predictions = model.predict(dict(sample_encodings))\n",
        "\n",
        "predicted_logits = predictions.logits\n",
        "predicted_classes = tf.argmax(predicted_logits, axis=1).numpy() # Get the index of the highest logit\n",
        "\n",
        "# Convert predicted class indices back to sentiment labels\n",
        "predicted_sentiments = [encoded_to_label[class_idx] for class_idx in predicted_classes]\n",
        "\n",
        "for sentence, sentiment in zip(sample_sentences, predicted_sentiments):\n",
        "    print(f\"Sentence: '{sentence}' -> Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "kshm0GXBcsdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After handling class imbalance, we could receive probbaly a better result.**"
      ],
      "metadata": {
        "id": "3HWCyZlg3pb4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zxPdN0phY5pP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}